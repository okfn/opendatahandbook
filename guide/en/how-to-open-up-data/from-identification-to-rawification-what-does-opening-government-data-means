---
title: From Identification to "Rawification": What Does Opening Government Data Mean?

authors:
 - *Jérôme Denis* 
Associate Professor
I3 (CNRS UMR 9217)
Telecom ParisTech
jerome.denis/at/telecom-paristech.fr
- *Samuel Goëta*
PhD Candidate, Telecom ParisTech
Board member, Open Knowledge France
samuel.goeta/at/telecom-paristech.fr

---

###Introduction
Open government data has been at the center of several transparency policies and has been established as a legal obligation in many countries. Debates and discussions about open data have mainly focused on its opportunities and its capacity to develop transparency and innovation. Yet, how government data are concretely “opened” remain largely overlooked, as if data was, in its very nature, an easy available and shareable commodity. Is it really so simple?
Rather, an exploration of the backrooms of open government data projects in several French administrations [1] shows that openness, far from being an intrinsic property of data, is the result of a specific work. Three main kinds of operations can be identified, which punctuate such work and lead to the production of open government data: identification, extraction and "rawification". They all show that data is not an available commodity within administrations that would be naturally ready for its public diffusion, mechanically producing transparency. This article will help reusers to understand how data are shaped to foster or restrain certain uses, policymakers to assess some of the hidden costs of open data policies and activists to get a glimpse at how their demands are processed.  

###Identification
We observed that, first of all, the staffs that are in charge of open projects within administrations engage in a process of identification. Here, open data project managers along with data producers identify datasets which will need to be released.
Yet, before even trying to set a "perimeter" of data that could be opened, the very existence of data and their location are their main issues. Thus, before asking “how are we going to open this dataset or this one?”, the persons in charge of implementing an open government data program are facing an even more abysmal interrogation: “what data do we possess?”
Such identification is a crucial and complex operation, which rarely summarizes to the more or less fastidious gathering of well-defined entities that one should simply chase within the organization. Even if some of the people we interviewed dream of an exhaustive inventory of all the data that are present in an administration, the gathering actually occurs as an uncertain exploration during which data identification is progressively settled, through interactions with internal services. Far from being mechanical, data inventory implies meetings and discussions, sometimes negotiations, which are linked to various matters. Among these, the degree of sensitivity of data, that is the risk its opening represents, is often crucial. The concern for transparency might encounter detrimental consequences led by the publication of certain data. Other issues such as the technical easiness of data extraction, a likely interest of the public or the existence of longitudinal records may determine what data will be actually selected to be opened.
Two aspects are important to understand what is at stake with this operation of identification. First, it has organizational consequences. These are not only specific types of data that are identified. Places within the organization are also designated in this process, as well as persons, who are set up as responsible of these data and their circulation. Second, as a process, data identification is more a matter of instantiation than it is of genuine designation. This identification process is generative. It engenders a certain reality (Law, 2009), a perimeter of data that is not only established as open (“openable”, in the first place) but also as “data” at all. 

###Extraction
Once the data candidate to opening are identified, one still have to “grasp” them, which is, again, not self-evident. Indeed, the data do not become available by their mere identification, most of them are encapsulated in databases and their "release" requires them to be extracted from the software that makes them visible to their users.
Relational databases provide dedicated professional interfaces, called “user views” (Codd, 1970). Aimed at simplifying the use of the database, these multiple views allow a variety of uses, while preventing users from accessing to the physical organization of the data (Castelle, 2013). The users are thus dependant on these views and rare are the databases equipped with an automatic extraction feature, which would allow gathering the data independently of the software interface. To allow this extraction, the technicians who are more or less part of the open data teams try to delve beyond visualization interfaces, in the guts of hard drives, at the roots of databases. To do so, they build small ad hoc pieces of software that can extract data from its “base”. Even if broad principles can be found in the way data is actually stocked on hard drives, this “physical view” is always specific. Extraction tools have thus to be custom made for the various generations and systems of database software that populate administrations.
Therefore, to implement an open government data policy, people must be able, once data identified, to bypass the databases one way or another and to harvest the data directly from their storage space. Such extraction work performs the second step of the progressive instantiation of data. It shows again that data is not naturally available in administrations. Rather, it is embedded in sociotechnical assemblages that are transformed and rearranged in the process of opening data.

###“Rawification”
Finally, the opening of government data also implies transformations. This is surprising, since open data policies and prophets all emphasize the importance for the data to be "raw" [2] . But, as two decades of studies in information infrastructures have shown, “raw data is an oxymoron”, an expression coined by Geoffrey Bowker (2005). This is precisely this oxymoron that is foregrounded by the term some open data people use to describe the work they do to bring data to openness: "rawification". Government data have to become raw before it can be "opened".
There is lot to say about the different operations that take part in the process of "rawification". Let's only highlight two of them here: cleaning and ungrounding. The first one is a well-known process in sciences (Edwards, 2010). Data needs to be cleaned. Whilst there are always "good organisational reasons for bad records" (Garfinkel & Bittner, 1967), once opened, the government data must display certain qualities. What remained unnoticed approximations or absences, or locally adjusted dimensions in original datasets, can be considered as mistakes, flaws in the new framework of their openness. Cleaning consist thus in erasing, adjusting and correcting these "errors".
But cleaned data does not mean legible data. A lot of the data that are daily used in administration is not comprehensible by lay people. To be used by everyone, they have to be delivered from the marks of the practices they initially were tied to. Therefore, people in charge of the "rawification" of data work at erasing such things as labels, comments, or colors in spreadsheets. They also translate some of the professional vocabulary used in numerous datasets, replacing words, combining categories and so forth. Through these processes, the government data are "ungrounded", that is they are made universally understandable and usable. Cleaning and ungrounding are the main operations that show the amount of work needed to (re)produce raw data.

###Conclusion
Our exploration of three of the main operations that punctuate the process of opening government data foregrounds practices that remain largely invisible in the master narratives of open data. It is by no means exhaustive though, and it invites to go further in the "surfacing of the invisible work" (Star, 1999) that takes place behind the scenes of open government data policies. Formatting the data or producing metadata are for instance other operations that are crucial to the opening process.
What this brief account highlights is data is progressively instantiated as open data, through a series of sociotechnical operations that have a certain "cost" and lies on specific competencies. We think it is essential to take such invisible work and workers into account and to acknowledge their role in the production of open government data, the quality and the usability of which are never an intrinsic feature.
 
###Footnotes
[1]   We build on an ethnographic enquiry made of direct observations, in-depth interviews and documents analysis in several French administrations involved in open government data projects. A more complete account of this enquiry has been published : Denis, Jerome and Goëta, Samuel, Exploration, Extraction and ‘Rawification’. The Shaping of Transparency in the Back Rooms of Open Data (February 28, 2014). Available at [SSRN](http://ssrn.com/abstract=2403069).
[2]   Raw data has been a central claim of the Open Government Data movements. Two central characters of this demand are Open Knowledge Foundation’s and W3C’s Tim Berners-Lee. Pollock, founder of the Open Knowledge Foundation, wrote a blog post [http://blog.okfn.org/2007/11/07/give-us-the-data-raw-and-give-it-to-us-now/] in 2007 asking governments to publish raw data before releasing websites and applications containing the data. He argued that, while interfaces age quickly, raw data in open standards can always be re-used by the public. Pollock’s claim “Give us the data raw, give us the data now” inspired Tim Berners-Lee to urge governments in his 2009 TED Talk “we want raw data” 

###References
Bowker, G.C. 2005, Memory Practices in the Sciences. Cambridge, MIT Press.
Castelle, M. 2013, “Relational and Non-Relational Models in the Entextualization of Bureaucracy” Computational Culture (3).
Codd, E. F. 1970, “A Relational Model of Data for Large Shared Data Banks” Communications of the ACM 13(6) 377–3687.
Edwards, P. 2010, A Vast Machine. Computer Models, Climate Data, and the Politics of Global Warming. Cambridge, MIT Press.
Garfinkel, H. & Bittner, E. 1967, “Good organizational reasons for bad clinic records”, in Garfinkel H. Studies in Ethnnomethodology. Englewood-cliffs, Prentice-Hall, 186-207.
Law, J. 2009, “Seeing Like a Survey” Cultural Sociology 3(2) 239–256.
Star, S.L. 1999, “The Ethnography of Infrastructure”, American Behavioral Scientist, vol. 43 (3) 377-391.

